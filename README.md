# Microsoft Fabric Sales Analytics

[ðŸ‡¬ðŸ‡§ English](#english) | [ðŸ‡«ðŸ‡· FranÃ§ais](#franÃ§ais)

---

## Quick Links
- **Star schema:** [Data Model](#data-model-star-schema)
- **Pipeline (Fabric):** [Data Pipeline](#data-pipeline-microsoft-fabric)
- **Power BI dashboards:** [Power BI Reports](#power-bi-reports)

---

# English

## Project Overview
This repository showcases an **end-to-end analytics solution built with Microsoft Fabric**. The project simulates an e-commerce sales analytics platform where **daily CSV files** are ingested into a Lakehouse, transformed using **Apache Spark**, stored as **Delta tables**, modeled using a **Star Schema**, and finally visualized in **Power BI**.

**Main goals**
- Build a modern BI pipeline using Microsoft Fabric (OneLake + Lakehouse + Spark + Pipelines)
- Model clean analytical tables (Fact + Dimensions) for reporting
- Deliver interactive dashboards with actionable KPIs

---

## Repository Structure
- `architecture/` â†’ data model screenshots (Star Schema)
- `data/sample/` â†’ small sample CSV file(s) for demonstration
- `fabric-pipeline/screenshots/` â†’ pipeline screenshots (orchestration + runs)
- `notebooks/` â†’ Spark notebooks (transformation + validation)
- `sql/` â†’ warehouse / staging / SCD scripts
- `powerbi/screenshots/` â†’ report screenshots

---

## Architecture
**Flow:** Daily CSV â†’ Lakehouse (OneLake) â†’ Spark transformations (Delta) â†’ Star Schema â†’ Power BI

## Medallion Architecture (Bronze / Silver / Gold)

I used a Medallion setup to keep the pipeline clean and easy to maintain:

**Daily CSV â†’ Bronze (raw) â†’ Silver (clean) â†’ Gold (Star Schema) â†’ Semantic model â†’ Power BI**

| Layer | Where | What it contains | Produced by |
|------|------|-------------------|------------|
| **Bronze** | Lakehouse (Raw) | Raw CSV + `sales_orders_raw` | ingestion (pipeline) |
| **Silver** | Lakehouse (Curated) | Clean Delta table `sales_orders_clean` | `NB_Transform_Sales_Data` |
| **Gold** | Warehouse | Star Schema: `FactSales`, `DimDate`, `DimProduct`, `DimChannel` | SQL/Warehouse step |



---

## Data Model (Star Schema)
The warehouse follows a **Star Schema** design:
- **Fact table:** `FactSales` (transactions / measures)
- **Dimensions:** `DimDate`, `DimProduct`, `DimChannel`

![Star Schema](architecture/star_schema.png)

---

## Data Pipeline (Microsoft Fabric)
The pipeline orchestrates the processing workflow:
1. **Get Metadata**: checks whether new daily CSV files exist
2. **Condition**: stops cleanly if no files are found
3. **Set Variable**: stores the execution timestamp for traceability
4. **Notebook**: runs Spark transformations (raw â†’ curated Delta tables)
5. **Script**: optional SQL step (e.g., SCD merge / staging)
6. **Notebook**: runs validation checks
7. **Web activity**: sends a completion notification

### Pipeline canvas
![Pipeline Canvas](fabric-pipeline/screenshots/pipeline_canvas.png)

### Example run (success)
![Pipeline Run Success](fabric-pipeline/screenshots/pipeline_run_success.png)

### Example SQL/SCD script step
![Pipeline SCD Script](fabric-pipeline/screenshots/pipeline_scd_script.png)

---

## SQL Layer (Warehouse + SCD)

The `sql/` folder contains the Warehouse logic used to load and maintain dimensions using **Slowly Changing Dimensions (SCD)** patterns.

### SCD implementations

| Dimension | Type | Goal | Where | How it works |
|---|---:|---|---|---|
| `DimChannel` | **SCD Type 1** | Keep the latest channel values | Warehouse | `MERGE` into the dimension (overwrite / insert if new) |
| `DimProduct_SCD2` | **SCD Type 2** | Track full history of product attributes (e.g., category changes) | Warehouse | Close current row (`end_date`, `is_current=0`) and insert a new version (`start_date`, `is_current=1`) |



---

## Power BI Reports
The `powerbi/screenshots/` folder contains the final dashboards built on top of the Star Schema.

### Executive overview
![Executive Overview](powerbi/screenshots/Executive_overview.png)

### Channel & category performance
![Channel Product Performance](powerbi/screenshots/Channel_product_performance.png)

### Sales explorer (transaction/detail view)
![Sales Explorer](powerbi/screenshots/Sales_explorer.png)

### Time analysis
![Time Analysis](powerbi/screenshots/Time_analysis.png)

---

## KPIs Included
Examples of metrics available in the dashboards:
- **Total Revenue (Â£)**
- **Total Orders**
- **Units Sold**
- **Average Order Value (Â£)**
- Revenue & orders by **channel**, **product category**, and **time**

---

## Tech Stack
- **Microsoft Fabric** (OneLake, Lakehouse, Pipelines)
- **Apache Spark / PySpark** (transformations)
- **Delta Lake** (curated tables)
- **SQL** (warehouse / staging / merges)
- **Power BI** (dashboards)

---

# FranÃ§ais

## PrÃ©sentation du projet
Ce dÃ©pÃ´t prÃ©sente une **solution analytique de bout en bout dÃ©veloppÃ©e avec Microsoft Fabric**. Le projet simule un cas e-commerce oÃ¹ des **fichiers CSV quotidiens** sont ingÃ©rÃ©s dans un Lakehouse, transformÃ©s avec **Apache Spark**, stockÃ©s en **tables Delta**, modÃ©lisÃ©s en **schÃ©ma en Ã©toile (Star Schema)**, puis visualisÃ©s dans **Power BI**.

**Objectifs principaux**
- Construire un pipeline BI moderne avec Microsoft Fabric (OneLake + Lakehouse + Spark + Pipelines)
- Produire des tables analytiques propres (faits + dimensions) pour le reporting
- Livrer des dashboards interactifs avec des KPI exploitables

---

## Structure du dÃ©pÃ´t
- `architecture/` â†’ captures du modÃ¨le (Star Schema)
- `data/sample/` â†’ petit CSV dâ€™exemple
- `fabric-pipeline/screenshots/` â†’ captures du pipeline (orchestration + exÃ©cutions)
- `notebooks/` â†’ notebooks Spark (transformation + validation)
- `sql/` â†’ scripts SQL (staging / SCD / requÃªtes)
- `powerbi/screenshots/` â†’ captures Power BI

## Architecture Medallion (Bronze / Silver / Gold)

Jâ€™ai utilisÃ© une architecture Medallion pour garder le projet simple et maintenable :

**CSV quotidiens â†’ Bronze (raw) â†’ Silver (clean) â†’ Gold (Star Schema) â†’ ModÃ¨le sÃ©mantique â†’ Power BI**

| Couche | OÃ¹ | Contenu | Produit par |
|------|---|---------|------------|
| **Bronze** | Lakehouse (Raw) | CSV bruts + `sales_orders_raw` | ingestion (pipeline) |
| **Silver** | Lakehouse (Curated) | table Delta nettoyÃ©e `sales_orders_clean` | `NB_Transform_Sales_Data` |
| **Gold** | Warehouse | schÃ©ma en Ã©toile : `FactSales`, `DimDate`, `DimProduct`, `DimChannel` | Ã©tape SQL/Warehouse |





---

## ModÃ¨le de donnÃ©es (Star Schema)
Le Data Warehouse suit un **schÃ©ma en Ã©toile** :
- **Table de faits :** `FactSales`
- **Dimensions :** `DimDate`, `DimProduct`, `DimChannel`

![Star Schema](architecture/star_schema.png)

---

## Pipeline de donnÃ©es (Microsoft Fabric)
Le pipeline orchestre le traitement quotidien :
1. **Get Metadata** : vÃ©rifie la prÃ©sence de nouveaux fichiers
2. **Condition** : arrÃªt propre si aucun fichier
3. **Set Variable** : enregistre un timestamp dâ€™exÃ©cution
4. **Notebook** : transformations Spark (raw â†’ Delta)
5. **Script** : Ã©tape SQL optionnelle (ex : merge / SCD)
6. **Notebook** : contrÃ´les de validation
7. **Web** : notification de fin dâ€™exÃ©cution

### Canvas du pipeline
![Pipeline Canvas](fabric-pipeline/screenshots/pipeline_canvas.png)

### Exemple dâ€™exÃ©cution (succÃ¨s)
![Pipeline Run Success](fabric-pipeline/screenshots/pipeline_run_success.png)

### Exemple dâ€™Ã©tape SQL/SCD
![Pipeline SCD Script](fabric-pipeline/screenshots/pipeline_scd_script.png)

---

## Couche SQL (Warehouse + SCD)

Le dossier `sql/` contient la logique Warehouse utilisÃ©e pour charger et maintenir les dimensions via des patterns **SCD (Slowly Changing Dimensions)**.

### ImplÃ©mentations SCD

| Dimension | Type | Objectif | OÃ¹ | Principe |
|---|---:|---|---|---|
| `DimChannel` | **SCD Type 1** | Conserver uniquement lâ€™Ã©tat le plus rÃ©cent | Warehouse | `MERGE` dans la dimension (Ã©crasement / insertion si nouveau) |
| `DimProduct_SCD2` | **SCD Type 2** | Conserver lâ€™historique complet (ex : changement de catÃ©gorie produit) | Warehouse | Fermeture de la ligne courante (`end_date`, `is_current=0`) puis insertion dâ€™une nouvelle version (`start_date`, `is_current=1`) |




---

## Rapports Power BI
Les captures Power BI sont disponibles dans `powerbi/screenshots/`.

### Vue exÃ©cutive
![Executive Overview](powerbi/screenshots/Executive_overview.png)

### Performance par canal & catÃ©gorie
![Channel Product Performance](powerbi/screenshots/Channel_product_performance.png)

### Exploration des ventes (dÃ©tail des transactions)
![Sales Explorer](powerbi/screenshots/Sales_explorer.png)

### Analyse temporelle
![Time Analysis](powerbi/screenshots/Time_analysis.png)

---

## KPI disponibles
Exemples dâ€™indicateurs visibles dans les dashboards :
- **Chiffre dâ€™affaires total (Â£)**
- **Nombre total de commandes**
- **UnitÃ©s vendues**
- **Panier moyen (Â£)**
- Analyse par **canal**, **catÃ©gorie**, et **temps**

---

## Stack technique
- **Microsoft Fabric** (OneLake, Lakehouse, Pipelines)
- **Apache Spark / PySpark**
- **Delta Lake**
- **SQL**
- **Power BI**
