# Microsoft Fabric Sales Analytics

[üá¨üáß English](#english) | [üá´üá∑ Fran√ßais](#fran√ßais)

---

## Quick Links
- **Star schema:** [Data Model](#data-model-star-schema)
- **Pipeline (Fabric):** [Data Pipeline](#data-pipeline-microsoft-fabric)
- **Power BI dashboards:** [Power BI Reports](#power-bi-reports)

---

# English

## Project Overview
This repository showcases an **end-to-end analytics solution built with Microsoft Fabric**. The project simulates an e-commerce sales analytics platform where **daily CSV files** are ingested into a Lakehouse, transformed using **Apache Spark**, stored as **Delta tables**, modeled using a **Star Schema**, and finally visualized in **Power BI**.

**Main goals**
- Build a modern BI pipeline using Microsoft Fabric (OneLake + Lakehouse + Spark + Pipelines)
- Model clean analytical tables (Fact + Dimensions) for reporting
- Deliver interactive dashboards with actionable KPIs

---

## Repository Structure
- `architecture/` ‚Üí data model screenshots (Star Schema)
- `data/sample/` ‚Üí small sample CSV file(s) for demonstration
- `fabric-pipeline/screenshots/` ‚Üí pipeline screenshots (orchestration + runs)
- `notebooks/` ‚Üí Spark notebooks (transformation + validation)
- `sql/` ‚Üí warehouse / staging / SCD scripts
- `powerbi/screenshots/` ‚Üí report screenshots

---

## Architecture
**Flow:** Daily CSV ‚Üí Lakehouse (OneLake) ‚Üí Spark transformations (Delta) ‚Üí Star Schema ‚Üí Power BI

## Medallion Architecture (Bronze / Silver / Gold)

I used a Medallion setup to keep the pipeline clean and easy to maintain:

**Daily CSV ‚Üí Bronze (raw) ‚Üí Silver (clean) ‚Üí Gold (Star Schema) ‚Üí Semantic model ‚Üí Power BI**

| Layer | Where | What it contains | Produced by |
|------|------|-------------------|------------|
| **Bronze** | Lakehouse (Raw) | Raw CSV + `sales_orders_raw` | ingestion (pipeline) |
| **Silver** | Lakehouse (Curated) | Clean Delta table `sales_orders_clean` | `NB_Transform_Sales_Data` |
| **Gold** | Warehouse | Star Schema: `FactSales`, `DimDate`, `DimProduct`, `DimChannel` | SQL/Warehouse step |



---

## Data Model (Star Schema)
The warehouse follows a **Star Schema** design:
- **Fact table:** `FactSales` (transactions / measures)
- **Dimensions:** `DimDate`, `DimProduct`, `DimChannel`

![Star Schema](architecture/star_schema.png)

---

## Data Pipeline (Microsoft Fabric)
The pipeline orchestrates the processing workflow:
1. **Get Metadata**: checks whether new daily CSV files exist
2. **Condition**: stops cleanly if no files are found
3. **Set Variable**: stores the execution timestamp for traceability
4. **Notebook**: runs Spark transformations (raw ‚Üí curated Delta tables)
5. **Script**: optional SQL step (e.g., SCD merge / staging)
6. **Notebook**: runs validation checks
7. **Web activity**: sends a completion notification

### Pipeline canvas
![Pipeline Canvas](fabric-pipeline/screenshots/pipeline_canvas.png)

### Example run (success)
![Pipeline Run Success](fabric-pipeline/screenshots/pipeline_run_success.png)

### Example SQL/SCD script step
![Pipeline SCD Script](fabric-pipeline/screenshots/pipeline_scd_script.png)

---

## SQL Layer (Warehouse + SCD)

The `sql/` folder contains the Warehouse logic used to load and maintain dimensions using **Slowly Changing Dimensions (SCD)** patterns.

### SCD implementations

| Dimension | Type | Goal | Where | How it works |
|---|---:|---|---|---|
| `DimChannel` | **SCD Type 1** | Keep the latest channel values | Warehouse | `MERGE` into the dimension (overwrite / insert if new) |
| `DimProduct_SCD2` | **SCD Type 2** | Track full history of product attributes (e.g., category changes) | Warehouse | Close current row (`end_date`, `is_current=0`) and insert a new version (`start_date`, `is_current=1`) |



---

## Power BI Reports
The `powerbi/screenshots/` folder contains the final dashboards built on top of the Star Schema.

### Executive overview
![Executive Overview](powerbi/screenshots/Executive_overview.png)

### Channel & category performance
![Channel Product Performance](powerbi/screenshots/Channel_product_performance.png)

### Sales explorer (transaction/detail view)
![Sales Explorer](powerbi/screenshots/Sales_explorer.png)

### Time analysis
![Time Analysis](powerbi/screenshots/Time_analysis.png)

---

## KPIs Included
Examples of metrics available in the dashboards:
- **Total Revenue (¬£)**
- **Total Orders**
- **Units Sold**
- **Average Order Value (¬£)**
- Revenue & orders by **channel**, **product category**, and **time**

---

## Tech Stack
- **Microsoft Fabric** (OneLake, Lakehouse, Pipelines)
- **Apache Spark / PySpark** (transformations)
- **Delta Lake** (curated tables)
- **SQL** (warehouse / staging / merges)
- **Power BI** (dashboards)

---

# Fran√ßais

## Pr√©sentation du projet
Ce d√©p√¥t pr√©sente une **solution analytique de bout en bout d√©velopp√©e avec Microsoft Fabric**. Le projet simule un cas e-commerce o√π des **fichiers CSV quotidiens** sont ing√©r√©s dans un Lakehouse, transform√©s avec **Apache Spark**, stock√©s en **tables Delta**, mod√©lis√©s en **sch√©ma en √©toile (Star Schema)**, puis visualis√©s dans **Power BI**.

**Objectifs principaux**
- Construire un pipeline BI moderne avec Microsoft Fabric (OneLake + Lakehouse + Spark + Pipelines)
- Produire des tables analytiques propres (faits + dimensions) pour le reporting
- Livrer des dashboards interactifs avec des KPI exploitables

---

## Structure du d√©p√¥t
- `architecture/` ‚Üí captures du mod√®le (Star Schema)
- `data/sample/` ‚Üí petit CSV d‚Äôexemple
- `fabric-pipeline/screenshots/` ‚Üí captures du pipeline (orchestration + ex√©cutions)
- `notebooks/` ‚Üí notebooks Spark (transformation + validation)
- `sql/` ‚Üí scripts SQL (staging / SCD / requ√™tes)
- `powerbi/screenshots/` ‚Üí captures Power BI

## Architecture Medallion (Bronze / Silver / Gold)

J‚Äôai utilis√© une architecture Medallion pour garder le projet simple et maintenable :

**CSV quotidiens ‚Üí Bronze (raw) ‚Üí Silver (clean) ‚Üí Gold (Star Schema) ‚Üí Mod√®le s√©mantique ‚Üí Power BI**

| Couche | O√π | Contenu | Produit par |
|------|---|---------|------------|
| **Bronze** | Lakehouse (Raw) | CSV bruts + `sales_orders_raw` | ingestion (pipeline) |
| **Silver** | Lakehouse (Curated) | table Delta nettoy√©e `sales_orders_clean` | `NB_Transform_Sales_Data` |
| **Gold** | Warehouse | sch√©ma en √©toile : `FactSales`, `DimDate`, `DimProduct`, `DimChannel` | √©tape SQL/Warehouse |





---

## Mod√®le de donn√©es (Star Schema)
Le Data Warehouse suit un **sch√©ma en √©toile** :
- **Table de faits :** `FactSales`
- **Dimensions :** `DimDate`, `DimProduct`, `DimChannel`

![Star Schema](architecture/star_schema.png)

---

## Pipeline de donn√©es (Microsoft Fabric)
Le pipeline orchestre le traitement quotidien :
1. **Get Metadata** : v√©rifie la pr√©sence de nouveaux fichiers
2. **Condition** : arr√™t propre si aucun fichier
3. **Set Variable** : enregistre un timestamp d‚Äôex√©cution
4. **Notebook** : transformations Spark (raw ‚Üí Delta)
5. **Script** : √©tape SQL optionnelle (ex : merge / SCD)
6. **Notebook** : contr√¥les de validation
7. **Web** : notification de fin d‚Äôex√©cution

### Canvas du pipeline
![Pipeline Canvas](fabric-pipeline/screenshots/pipeline_canvas.png)

### Exemple d‚Äôex√©cution (succ√®s)
![Pipeline Run Success](fabric-pipeline/screenshots/pipeline_run_success.png)

### Exemple d‚Äô√©tape SQL/SCD
![Pipeline SCD Script](fabric-pipeline/screenshots/pipeline_scd_script.png)

---

## Couche SQL (Warehouse + SCD)

Le dossier `sql/` contient la logique Warehouse utilis√©e pour charger et maintenir les dimensions via des patterns **SCD (Slowly Changing Dimensions)**.

### Impl√©mentations SCD

| Dimension | Type | Objectif | O√π | Principe |
|---|---:|---|---|---|
| `DimChannel` | **SCD Type 1** | Conserver uniquement l‚Äô√©tat le plus r√©cent | Warehouse | `MERGE` dans la dimension (√©crasement / insertion si nouveau) |
| `DimProduct_SCD2` | **SCD Type 2** | Conserver l‚Äôhistorique complet (ex : changement de cat√©gorie produit) | Warehouse | Fermeture de la ligne courante (`end_date`, `is_current=0`) puis insertion d‚Äôune nouvelle version (`start_date`, `is_current=1`) |




---

## Rapports Power BI
Les captures Power BI sont disponibles dans `powerbi/screenshots/`.

### Vue ex√©cutive
![Executive Overview](powerbi/screenshots/Executive_overview.png)

### Performance par canal & cat√©gorie
![Channel Product Performance](powerbi/screenshots/Channel_product_performance.png)

### Exploration des ventes (d√©tail des transactions)
![Sales Explorer](powerbi/screenshots/Sales_explorer.png)

### Analyse temporelle
![Time Analysis](powerbi/screenshots/Time_analysis.png)

---

## KPI disponibles
Exemples d‚Äôindicateurs visibles dans les dashboards :
- **Chiffre d‚Äôaffaires total (¬£)**
- **Nombre total de commandes**
- **Unit√©s vendues**
- **Panier moyen (¬£)**
- Analyse par **canal**, **cat√©gorie**, et **temps**

---

## Stack technique
- **Microsoft Fabric** (OneLake, Lakehouse, Pipelines)
- **Apache Spark / PySpark**
- **Delta Lake**
-  **SQL**
- **Power BI**

  ---

## Limites actuelles 

Ce d√©p√¥t est une **d√©mo production-like** : il illustre l‚Äôarchitecture, le pipeline Fabric, le mod√®le en √©toile et les dashboards Power BI, mais il ne repr√©sente pas encore une cha√Æne d‚Äôexploitation compl√®te en continu.

### Pourquoi c‚Äôest encore une d√©mo
- **Donn√©es d‚Äôentr√©e simul√©es** : les CSV sont des *samples* (ajout√©s manuellement pour la d√©monstration).
- **Alimentation non garantie au quotidien** : le pipeline n‚Äôest pas  d√©clench√© par une arriv√©e automatique de fichiers chaque jour.

### √Ä quoi ressemblerait une version 100% professionnelle dans Fabric
- **Ingestion automatique** depuis une source stable (par ex. un dossier OneLake recevant `orders_YYYY-MM-DD.csv`, un stockage externe type ADLS/S3, ou une API/SaaS).  
- **Orchestration planifi√©e** : pipeline d√©clench√© par un **schedule daily** ou un trigger ‚Äúarrival-based‚Äù, avec un param√®tre `run_date` et une ex√©cution **idempotente** (pas de doublons si relanc√©).
- **Traitements incr√©mentaux** : Bronze append-only, Silver avec nettoyage/d√©doublonnage, Gold aliment√© de fa√ßon incr√©mentale (√©viter le full refresh syst√©matique).



