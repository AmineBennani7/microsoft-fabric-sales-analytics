# Microsoft Fabric Sales Analytics

[üá¨üáß English](#english) | [üá´üá∑ Fran√ßais](#fran√ßais)

---

## Quick Links
- **Star schema:** [Data Model](#data-model-star-schema)
- **Pipeline (Fabric):** [Data Pipeline](#data-pipeline-microsoft-fabric)
- **Power BI dashboards:** [Power BI Reports](#power-bi-reports)

---

# English

## Project Overview
This repository showcases an **end-to-end analytics solution built with Microsoft Fabric**. The project simulates an e-commerce sales analytics platform where **daily CSV files** are ingested into a Lakehouse, transformed using **Apache Spark**, stored as **Delta tables**, modeled using a **Star Schema**, and finally visualized in **Power BI**.

**Main goals**
- Build a modern BI pipeline using Microsoft Fabric (OneLake + Lakehouse + Spark + Pipelines)
- Model clean analytical tables (Fact + Dimensions) for reporting
- Deliver interactive dashboards with actionable KPIs

---

## Repository Structure
- `architecture/` ‚Üí data model screenshots (Star Schema)
- `data/sample/` ‚Üí small sample CSV file(s) for demonstration
- `fabric-pipeline/screenshots/` ‚Üí pipeline screenshots (orchestration + runs)
- `notebooks/` ‚Üí Spark notebooks (transformation + validation)
- `sql/` ‚Üí warehouse / staging / SCD scripts
- `powerbi/screenshots/` ‚Üí report screenshots

---

## Architecture
**Flow:** Daily CSV ‚Üí Lakehouse (OneLake) ‚Üí Spark transformations (Delta) ‚Üí Star Schema ‚Üí Power BI

## Medallion Architecture (Bronze / Silver / Gold)

This project follows a **Medallion Architecture** pattern to structure the analytics lifecycle:

**Raw CSV (daily sales files)**
‚Üí **Bronze (Lakehouse - Raw)**
‚Üí **Silver (Lakehouse - Clean/Curated)**
‚Üí **Gold (Warehouse - Star Schema)**
‚Üí Semantic Model
‚Üí Power BI

### Bronze (Raw)
**Purpose:** ingest data exactly as received (append-only), preserve lineage and enable reprocessing.  
**Where in this repo / Fabric:** Lakehouse raw zone (e.g., `raw/orders/`) containing daily CSV files such as:
- `data/sample/orders_2026-01-01.csv`

Typical table / folder:
- `sales_orders_raw` (raw ingestion)

### Silver (Clean / Curated)
**Purpose:** clean and standardize the raw data into a consistent, analytics-ready format.  
Includes:
- schema enforcement and type casting (dates, numeric fields)
- basic data quality checks (nulls, duplicates, invalid rows)
- standardized columns and business logic preparation

Typical table:
- `sales_orders_clean` (cleaned Delta table)

### Gold (Business-ready)
**Purpose:** deliver business-friendly tables optimized for BI and analytics.  
In this project, the **Gold layer is the Warehouse modeled as a Star Schema**:
- `FactSales`
- `DimProduct`
- `DimDate`
- `DimChannel`

---

## Data Model (Star Schema)
The warehouse follows a **Star Schema** design:
- **Fact table:** `FactSales` (transactions / measures)
- **Dimensions:** `DimDate`, `DimProduct`, `DimChannel`

![Star Schema](architecture/star_schema.png)

---

## Data Pipeline (Microsoft Fabric)
The pipeline orchestrates the processing workflow:
1. **Get Metadata**: checks whether new daily CSV files exist
2. **Condition**: stops cleanly if no files are found
3. **Set Variable**: stores the execution timestamp for traceability
4. **Notebook**: runs Spark transformations (raw ‚Üí curated Delta tables)
5. **Script**: optional SQL step (e.g., SCD merge / staging)
6. **Notebook**: runs validation checks
7. **Web activity**: sends a completion notification

### Pipeline canvas
![Pipeline Canvas](fabric-pipeline/screenshots/pipeline_canvas.png)

### Example run (success)
![Pipeline Run Success](fabric-pipeline/screenshots/pipeline_run_success.png)

### Example SQL/SCD script step
![Pipeline SCD Script](fabric-pipeline/screenshots/pipeline_scd_script.png)

---

## SQL Layer (Warehouse + SCD)
The `sql/` folder contains SQL scripts used to:
- reset / create staging tables
- implement merge logic (e.g., **SCD Type 1**)
- run demo / validation queries

This demonstrates practical data warehousing patterns (staging, merges, analytical modeling).

---

## Power BI Reports
The `powerbi/screenshots/` folder contains the final dashboards built on top of the Star Schema.

### Executive overview
![Executive Overview](powerbi/screenshots/Executive_overview.png)

### Channel & category performance
![Channel Product Performance](powerbi/screenshots/Channel_product_performance.png)

### Sales explorer (transaction/detail view)
![Sales Explorer](powerbi/screenshots/Sales_explorer.png)

### Time analysis
![Time Analysis](powerbi/screenshots/Time_analysis.png)

---

## KPIs Included
Examples of metrics available in the dashboards:
- **Total Revenue (¬£)**
- **Total Orders**
- **Units Sold**
- **Average Order Value (¬£)**
- Revenue & orders by **channel**, **product category**, and **time**

---

## Tech Stack
- **Microsoft Fabric** (OneLake, Lakehouse, Pipelines)
- **Apache Spark / PySpark** (transformations)
- **Delta Lake** (curated tables)
- **SQL** (warehouse / staging / merges)
- **Power BI** (dashboards)

---

# Fran√ßais

## Pr√©sentation du projet
Ce d√©p√¥t pr√©sente une **solution analytique de bout en bout d√©velopp√©e avec Microsoft Fabric**. Le projet simule un cas e-commerce o√π des **fichiers CSV quotidiens** sont ing√©r√©s dans un Lakehouse, transform√©s avec **Apache Spark**, stock√©s en **tables Delta**, mod√©lis√©s en **sch√©ma en √©toile (Star Schema)**, puis visualis√©s dans **Power BI**.

**Objectifs principaux**
- Construire un pipeline BI moderne avec Microsoft Fabric (OneLake + Lakehouse + Spark + Pipelines)
- Produire des tables analytiques propres (faits + dimensions) pour le reporting
- Livrer des dashboards interactifs avec des KPI exploitables

---

## Structure du d√©p√¥t
- `architecture/` ‚Üí captures du mod√®le (Star Schema)
- `data/sample/` ‚Üí petit CSV d‚Äôexemple
- `fabric-pipeline/screenshots/` ‚Üí captures du pipeline (orchestration + ex√©cutions)
- `notebooks/` ‚Üí notebooks Spark (transformation + validation)
- `sql/` ‚Üí scripts SQL (staging / SCD / requ√™tes)
- `powerbi/screenshots/` ‚Üí captures Power BI

## Architecture Medallion (Bronze / Silver / Gold)

Ce projet suit une **architecture Medallion** (Bronze/Silver/Gold) pour organiser le cycle de vie des donn√©es :

**CSV bruts (ventes quotidiennes)**
‚Üí **Bronze (Lakehouse - Raw)**
‚Üí **Silver (Lakehouse - Clean/Curated)**
‚Üí **Gold (Warehouse - Star Schema)**
‚Üí Mod√®le s√©mantique
‚Üí Power BI

### Bronze (Raw)
**Objectif :** ing√©rer les donn√©es telles quelles (append-only), conserver la tra√ßabilit√© et permettre le retraitement.  
**Dans Fabric / dans ce repo :** zone raw du Lakehouse (ex : `raw/orders/`) avec les CSV quotidiens, par exemple :
- `data/sample/orders_2026-01-01.csv`

Table / dossier typique :
- `sales_orders_raw`

### Silver (Clean / Curated)
**Objectif :** nettoyer et standardiser les donn√©es afin d‚Äôobtenir un format coh√©rent pr√™t pour l‚Äôanalyse.  
Contient :
- typage et normalisation (dates, num√©riques)
- contr√¥les qualit√© (valeurs nulles, doublons, lignes invalides)
- colonnes standardis√©es et logique m√©tier de base

Table typique :
- `sales_orders_clean` (table Delta nettoy√©e)

### Gold (Business-ready)
**Objectif :** produire des tables orient√©es m√©tier, optimis√©es pour la BI.  
Dans ce projet, la couche **Gold correspond au Warehouse en sch√©ma en √©toile** :
- `FactSales`
- `DimProduct`
- `DimDate`
- `DimChannel`



---

## Mod√®le de donn√©es (Star Schema)
Le Data Warehouse suit un **sch√©ma en √©toile** :
- **Table de faits :** `FactSales`
- **Dimensions :** `DimDate`, `DimProduct`, `DimChannel`

![Star Schema](architecture/star_schema.png)

---

## Pipeline de donn√©es (Microsoft Fabric)
Le pipeline orchestre le traitement quotidien :
1. **Get Metadata** : v√©rifie la pr√©sence de nouveaux fichiers
2. **Condition** : arr√™t propre si aucun fichier
3. **Set Variable** : enregistre un timestamp d‚Äôex√©cution
4. **Notebook** : transformations Spark (raw ‚Üí Delta)
5. **Script** : √©tape SQL optionnelle (ex : merge / SCD)
6. **Notebook** : contr√¥les de validation
7. **Web** : notification de fin d‚Äôex√©cution

### Canvas du pipeline
![Pipeline Canvas](fabric-pipeline/screenshots/pipeline_canvas.png)

### Exemple d‚Äôex√©cution (succ√®s)
![Pipeline Run Success](fabric-pipeline/screenshots/pipeline_run_success.png)

### Exemple d‚Äô√©tape SQL/SCD
![Pipeline SCD Script](fabric-pipeline/screenshots/pipeline_scd_script.png)

---

## Couche SQL (Warehouse + SCD)
Le dossier `sql/` contient des scripts pour :
- cr√©er / r√©initialiser les tables de staging
- impl√©menter des merges (ex : **SCD Type 1**)
- ex√©cuter des requ√™tes de d√©monstration / validation

---

## Rapports Power BI
Les captures Power BI sont disponibles dans `powerbi/screenshots/`.

### Vue ex√©cutive
![Executive Overview](powerbi/screenshots/Executive_overview.png)

### Performance par canal & cat√©gorie
![Channel Product Performance](powerbi/screenshots/Channel_product_performance.png)

### Exploration des ventes (d√©tail des transactions)
![Sales Explorer](powerbi/screenshots/Sales_explorer.png)

### Analyse temporelle
![Time Analysis](powerbi/screenshots/Time_analysis.png)

---

## KPI disponibles
Exemples d‚Äôindicateurs visibles dans les dashboards :
- **Chiffre d‚Äôaffaires total (¬£)**
- **Nombre total de commandes**
- **Unit√©s vendues**
- **Panier moyen (¬£)**
- Analyse par **canal**, **cat√©gorie**, et **temps**

---

## Stack technique
- **Microsoft Fabric** (OneLake, Lakehouse, Pipelines)
- **Apache Spark / PySpark**
- **Delta Lake**
- **SQL**
- **Power BI**
