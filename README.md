# Microsoft Fabric Sales Analytics

[ðŸ‡¬ðŸ‡§ English](#english) | [ðŸ‡«ðŸ‡· FranÃ§ais](#franÃ§ais)

---

## Quick Links
- **Star schema:** [Data Model](#data-model-star-schema)
- **Pipeline (Fabric):** [Data Pipeline](#data-pipeline-microsoft-fabric)
- **Power BI dashboards:** [Power BI Reports](#power-bi-reports)

---

# English

## Project Overview
This repository showcases an **end-to-end analytics solution built with Microsoft Fabric**. The project simulates an e-commerce sales analytics platform where **daily CSV files** are ingested into a Lakehouse, transformed using **Apache Spark**, stored as **Delta tables**, modeled using a **Star Schema**, and finally visualized in **Power BI**.

**Main goals**
- Build a modern BI pipeline using Microsoft Fabric (OneLake + Lakehouse + Spark + Pipelines)
- Model clean analytical tables (Fact + Dimensions) for reporting
- Deliver interactive dashboards with actionable KPIs

---

## Repository Structure
- `architecture/` â†’ data model screenshots (Star Schema)
- `data/sample/` â†’ small sample CSV file(s) for demonstration
- `fabric-pipeline/screenshots/` â†’ pipeline screenshots (orchestration + runs)
- `notebooks/` â†’ Spark notebooks (transformation + validation)
- `sql/` â†’ warehouse / staging / SCD scripts
- `powerbi/screenshots/` â†’ report screenshots

---

## Architecture
**Flow:** Daily CSV â†’ Lakehouse (OneLake) â†’ Spark transformations (Delta) â†’ Star Schema â†’ Power BI

---

## Data Model (Star Schema)
The warehouse follows a **Star Schema** design:
- **Fact table:** `FactSales` (transactions / measures)
- **Dimensions:** `DimDate`, `DimProduct`, `DimChannel`

![Star Schema](architecture/star_schema.png)

---

## Data Pipeline (Microsoft Fabric)
The pipeline orchestrates the processing workflow:
1. **Get Metadata**: checks whether new daily CSV files exist
2. **Condition**: stops cleanly if no files are found
3. **Set Variable**: stores the execution timestamp for traceability
4. **Notebook**: runs Spark transformations (raw â†’ curated Delta tables)
5. **Script**: optional SQL step (e.g., SCD merge / staging)
6. **Notebook**: runs validation checks
7. **Web activity**: sends a completion notification

### Pipeline canvas
![Pipeline Canvas](fabric-pipeline/screenshots/pipeline_canvas.png)

### Example run (success)
![Pipeline Run Success](fabric-pipeline/screenshots/pipeline_run_success.png)

### Example SQL/SCD script step
![Pipeline SCD Script](fabric-pipeline/screenshots/pipeline_scd_script.png)

---

## SQL Layer (Warehouse + SCD)
The `sql/` folder contains SQL scripts used to:
- reset / create staging tables
- implement merge logic (e.g., **SCD Type 1**)
- run demo / validation queries

This demonstrates practical data warehousing patterns (staging, merges, analytical modeling).

---

## Power BI Reports
The `powerbi/screenshots/` folder contains the final dashboards built on top of the Star Schema.

### Executive overview
![Executive Overview](powerbi/screenshots/Executive_overview.png)

### Channel & category performance
![Channel Product Performance](powerbi/screenshots/Channel_product_performance.png)

### Sales explorer (transaction/detail view)
![Sales Explorer](powerbi/screenshots/Sales_explorer.png)

### Time analysis
![Time Analysis](powerbi/screenshots/Time_analysis.png)

---

## KPIs Included
Examples of metrics available in the dashboards:
- **Total Revenue (Â£)**
- **Total Orders**
- **Units Sold**
- **Average Order Value (Â£)**
- Revenue & orders by **channel**, **product category**, and **time**

---

## Tech Stack
- **Microsoft Fabric** (OneLake, Lakehouse, Pipelines)
- **Apache Spark / PySpark** (transformations)
- **Delta Lake** (curated tables)
- **SQL** (warehouse / staging / merges)
- **Power BI** (dashboards)

---

# FranÃ§ais

## PrÃ©sentation du projet
Ce dÃ©pÃ´t prÃ©sente une **solution analytique de bout en bout dÃ©veloppÃ©e avec Microsoft Fabric**. Le projet simule un cas e-commerce oÃ¹ des **fichiers CSV quotidiens** sont ingÃ©rÃ©s dans un Lakehouse, transformÃ©s avec **Apache Spark**, stockÃ©s en **tables Delta**, modÃ©lisÃ©s en **schÃ©ma en Ã©toile (Star Schema)**, puis visualisÃ©s dans **Power BI**.

**Objectifs principaux**
- Construire un pipeline BI moderne avec Microsoft Fabric (OneLake + Lakehouse + Spark + Pipelines)
- Produire des tables analytiques propres (faits + dimensions) pour le reporting
- Livrer des dashboards interactifs avec des KPI exploitables

---

## Structure du dÃ©pÃ´t
- `architecture/` â†’ captures du modÃ¨le (Star Schema)
- `data/sample/` â†’ petit CSV dâ€™exemple
- `fabric-pipeline/screenshots/` â†’ captures du pipeline (orchestration + exÃ©cutions)
- `notebooks/` â†’ notebooks Spark (transformation + validation)
- `sql/` â†’ scripts SQL (staging / SCD / requÃªtes)
- `powerbi/screenshots/` â†’ captures Power BI

---

## ModÃ¨le de donnÃ©es (Star Schema)
Le Data Warehouse suit un **schÃ©ma en Ã©toile** :
- **Table de faits :** `FactSales`
- **Dimensions :** `DimDate`, `DimProduct`, `DimChannel`

![Star Schema](architecture/star_schema.png)

---

## Pipeline de donnÃ©es (Microsoft Fabric)
Le pipeline orchestre le traitement quotidien :
1. **Get Metadata** : vÃ©rifie la prÃ©sence de nouveaux fichiers
2. **Condition** : arrÃªt propre si aucun fichier
3. **Set Variable** : enregistre un timestamp dâ€™exÃ©cution
4. **Notebook** : transformations Spark (raw â†’ Delta)
5. **Script** : Ã©tape SQL optionnelle (ex : merge / SCD)
6. **Notebook** : contrÃ´les de validation
7. **Web** : notification de fin dâ€™exÃ©cution

### Canvas du pipeline
![Pipeline Canvas](fabric-pipeline/screenshots/pipeline_canvas.png)

### Exemple dâ€™exÃ©cution (succÃ¨s)
![Pipeline Run Success](fabric-pipeline/screenshots/pipeline_run_success.png)

### Exemple dâ€™Ã©tape SQL/SCD
![Pipeline SCD Script](fabric-pipeline/screenshots/pipeline_scd_script.png)

---

## Couche SQL (Warehouse + SCD)
Le dossier `sql/` contient des scripts pour :
- crÃ©er / rÃ©initialiser les tables de staging
- implÃ©menter des merges (ex : **SCD Type 1**)
- exÃ©cuter des requÃªtes de dÃ©monstration / validation

---

## Rapports Power BI
Les captures Power BI sont disponibles dans `powerbi/screenshots/`.

### Vue exÃ©cutive
![Executive Overview](powerbi/screenshots/Executive_overview.png)

### Performance par canal & catÃ©gorie
![Channel Product Performance](powerbi/screenshots/Channel_product_performance.png)

### Exploration des ventes (dÃ©tail des transactions)
![Sales Explorer](powerbi/screenshots/Sales_explorer.png)

### Analyse temporelle
![Time Analysis](powerbi/screenshots/Time_analysis.png)

---

## KPI disponibles
Exemples dâ€™indicateurs visibles dans les dashboards :
- **Chiffre dâ€™affaires total (Â£)**
- **Nombre total de commandes**
- **UnitÃ©s vendues**
- **Panier moyen (Â£)**
- Analyse par **canal**, **catÃ©gorie**, et **temps**

---

## Stack technique
- **Microsoft Fabric** (OneLake, Lakehouse, Pipelines)
- **Apache Spark / PySpark**
- **Delta Lake**
- **SQL**
- **Power BI**
